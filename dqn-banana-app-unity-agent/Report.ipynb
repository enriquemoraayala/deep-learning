{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1 (Banana Collector) Report\n",
    "\n",
    "## Content\n",
    "\n",
    "In this report you will find the following sections:\n",
    "\n",
    "* Introduction\n",
    "* Algorithm\n",
    "* Model architecture\n",
    "* Training procedure\n",
    "* Agent performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This report explains the implementation and training of a RL based agent that is able to solve the Banana Collector problem, please see the README.md file for the problem details.\n",
    "\n",
    "## Algorithm\n",
    "\n",
    "In order to solve the problem the RL Agent has been trained with the DQN algorithm using Experience Replay and Fixed Q-Targets techniques.\n",
    "\n",
    "### DQN Algorithm\n",
    "\n",
    "The general expression of the DQN algorith is:\n",
    "\n",
    "![DQN Algorithm](images/dqn_algorithm.png)\n",
    "\n",
    "Please, read the [research paper](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf) for the complete details of the algorithm.\n",
    "\n",
    "It is also important to understand the two key techniques applied in order to improve the algorithm performance.\n",
    "\n",
    "#### Experience Replay\n",
    "\n",
    "When the agent interacts with the environment, the sequence of experience tuples can be highly correlated. The naive Q-learning algorithm that learns from each of these experience tuples in sequential order runs the risk of getting swayed by the effects of this correlation. By instead keeping track of a replay buffer and using experience replay to sample from the buffer at random, we can prevent action values from oscillating or diverging catastrophically.\n",
    "\n",
    "The replay buffer contains a collection of experience tuples $({S}, {A}, {R}, {S'})$. The tuples are gradually added to the buffer as we are interacting with the environment.\n",
    "\n",
    "#### Fixed Q-Targets\n",
    "\n",
    "In Q-Learning, we update a guess with a guess, and this can potentially lead to harmful correlations. To avoid this, we can update the parameters ww in the network $\\hat{q}$ to better approximate the action value corresponding to state SS and action AA with the following update rule:\n",
    "\n",
    "![DQN Algorithm](images/fixed_q-targets.png)\n",
    "\n",
    "where ${w^-}$ are the weights of a separate target network that are not changed during the learning step, and $({S}, {A}, {R}, {S'})$ is an experience tuple.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "\n",
    "The network architecture used for the DQN Algorithm is composed by three fully connected layers, concretelly:\n",
    "\n",
    "```python\n",
    "self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "```\n",
    "\n",
    "where:\n",
    "\n",
    "state_size = 37  \n",
    "fc1_units = 64  \n",
    "fc2_units = 64  \n",
    "action_size = 4  \n",
    "\n",
    "The activation function selected in this case is ReLU:\n",
    "\n",
    "```python\n",
    "def forward(self, state):\n",
    "    x = F.relu(self.fc1(state))\n",
    "    x = F.relu(self.fc2(x))\n",
    "    return self.fc3(x)\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Procedure\n",
    "\n",
    "\n",
    "In order to train and/or run the Agent, just follow the **Navigation.ipynb** Jupyter Notebook.  \n",
    "\n",
    "When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```\n",
    "\n",
    "For this training the hyperparameters selected are:\n",
    "\n",
    "n_episodes = 2000 (actually not needed)    \n",
    "max_t = 1000  \n",
    "eps_start = 1.0  \n",
    "eps_end = 0.01  \n",
    "eps_decay = 0.995  \n",
    "fc1_units = 64  \n",
    "fc2_units = 64  \n",
    "buffer_size = 10000 (replay buffer size)  \n",
    "batch_size = 64  \n",
    "gamma = 0.99  \n",
    "tau = 0.001  \n",
    "lr = 0.0005  \n",
    "update_every = 4 (how often to update the network)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Performance\n",
    "\n",
    "The agent solves the problem in 389 episodes (basically because the high number of the max_t parameter).\n",
    "\n",
    "![Agent Performance](images/agent_performance.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
